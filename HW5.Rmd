---
title: 'Machine Learning 2019: Tree-Based Methods'
author: "Tayler Sindhu"
date: "10/28/2019"
output: html_document
---

## Homework

**1. Attempt a regression tree-based method (not covered in this tutorial) on a reasonable dataset of your choice. Explain the results.**

## Question 1: Cubist Model
```{r set up, include=FALSE}
library(mlbench)
library(Cubist)
library(caret)
library(ipred)
library(gbm)
```
```{r}
# set random seed throughout markdown for reproducibility
knitr::opts_chunk$set(echo = TRUE, cache=T)
set.seed(35345)
```

```{r Question 1}
data("Vehicle")

# Cubist tree-based model using vehicle compactness as outcome
# Regression

# make training and test sets
train_size <- floor(0.75 * nrow(Vehicle))
train_pos <- sample(seq_len(nrow(Vehicle)), size = train_size)

train_vehicle <- Vehicle[train_pos, ]
test_vehicle <- Vehicle[-train_pos, ]

# use compactness (Comp) as dependent variable
# Make model
cubist <- cubist(train_vehicle[,c(2:19)],train_vehicle[,1], control=cubistControl())
cubist

# Visualize model according to rules
dotplot(cubist, main="Conditions")

# Summarize model
summary(cubist)

# predict on test set
test.predict <- predict(cubist, test_vehicle)

# R^2 value
cor(test.predict, test_vehicle$Comp)^2

# RMSE per documentation
sqrt(mean((test.predict - test_vehicle$Comp)^2))

# Plot actual vs. predicted values
ggplot() +
  geom_point(data = test_vehicle, aes(x=test.predict, y=Comp)) +
  xlab("Observed Compactness") +
  ylab("Predicted Compactness") + 
  ggtitle("Cubist model: Observed Compactness vs. Predicted Compactness") +
  theme(plot.title = element_text(hjust = 0.5))
```


###Cubist Model: Results Explanation
 This cubist model predicts the compactness of a vehicle type based on other vehicle characteristics. It is a form of regression tree in which trees are converted to rules. The model incorporates pruning, and a boosting-like procedure when the number of committees is greater than one. The final model used 1 committee and 7 rules. Visually, there is reasonably strong correlation between observed and predicted glucose values, and the R squared value is reasonably high at 0.85, and the RMSE is 3.20. The Holl.ra (hollows ratio) variable and Kurt.Maxis (kurtosis about major axis) variable were used in all of the models used to create the final model.
 

**2. Attempt both a bagging and boosting method on a reasonable dataset of your choice. Explain the results.**

## Question 2.1: Bagging Method
```{r Question 2.1}
# bagging method
# Classification

# Continuing to use vehicle dataset, but using vehicle class (Class) as outcome.
# Set model to predict vehicle class
bg <- bagging(Class ~ ., data=Vehicle)
predict.bg <- predict(bg, test_vehicle)

# Looking at variable importance averaged over all bagged trees
varImp(bg)

# Confusion matrix on test set to evaluate error 
#There are multiple classes
confusionMatrix(test_vehicle$Class, predict.bg)
```
### Bagging Method: Results Explanation
The bagging model showed a 98.2% accuracy in predicting vehical class based on other vehicle variables. In this model, the scaled variance along major axis (Sc.Var.maxis) was the most important variable (based on the decrease in residual sum of squares).

## Question 2.2: Boosting Method
```{r Question 2.2, warning=FALSE}
# boosting method

# Regression, again using compactness (Compact) as outcome.
bst <- gbm(Comp ~ ., data = train_vehicle, distribution = "gaussian", n.trees = 10000)
bst

# Looking at variable importance using bar plot showing all predictors
variable_imp <- summary.gbm(bst, plotit=FALSE, order=TRUE)

ggplot(data = variable_imp, aes(var, rel.inf))+
  geom_bar(stat="identity")+ 
  theme(axis.text.x = element_text(angle = 300))

# Visualize marginal effect of most important variable
plot.gbm(bst, i.var="Sc.Var.Maxis")

# Predict using test set, using code in class
n.trees = seq(from = 100, to = 10000, by = 0100)
predmat_veh <- predict(bst, newdata= test_vehicle, n.trees=n.trees) 

# Boosting Error Plot, using code in class
bst.err <- with(train_vehicle, apply( (predmat_veh - Comp)^2, 2, mean) )
plot(n.trees, bst.err, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(bst.err), col = "red")

# 1500 trees resulted in the least mean squared error, so will rerun prediction function using this value.
predmat_veh <- predict(bst, newdata= test_vehicle, n.trees= 1500) 

# R squared for test set
cor(predmat_veh, test_vehicle$Comp)^2

# RMSE for test set
sqrt(mean((predmat_veh - test_vehicle$Comp)^2))

# Plot actual vs. predicted values
ggplot() +
  geom_point(data = test_vehicle, aes(x=predmat_veh, y=Comp)) +
  xlab("Observed Compactness") +
  ylab("Predicted Compactness") + 
  ggtitle("Boosting Method: Observed Compactness vs. Predicted Compactness") +
  theme(plot.title = element_text(hjust = 0.5))
```

### Boosting Method: Results Explanation
This model performed slightly worse than the cubist model in ways such as the R-squared value (0.83 versus 0.85). The RMSE was also slightly higher (3.37 versus 3.20.) The scaled variance along major axis (Sc.Var.maxis) was the most important variable by far (accounts for the most reduction of squared error.) The least mean squared error occured with 1500 trees (when evaluating intervals of 100 from 100 to 10,000 trees), and all variables were important (in terms of "reduction of predictive performance").



